{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchrl\n",
    "!pip3 install gymnasium[mujoco]\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "from tensordict.nn import TensorDictModule, InteractionType\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "\n",
    "# Tensordict modules\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import TransformedEnv\n",
    "from torchrl.envs.libs.gym import GymEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.envs.transforms import DoubleToFloat\n",
    "\n",
    "# Model and policy\n",
    "from torchrl.modules import MLP, ProbabilisticActor, TanhNormal, ValueOperator\n",
    "# Loss\n",
    "from torchrl.objectives import SoftUpdate\n",
    "from torchrl.objectives.sac import SACLoss\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(device)\n",
    "gym_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration\n",
    "n_iters = 4*200  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 20  # Number of optimization steps per training iteration\n",
    "minibatch_size = 300  # Size of the mini-batches in each optimization step\n",
    "lr_actor = 6e-4  # Learning rate\n",
    "lr_critic = lr_actor  # Learning rate\n",
    "\n",
    "# SAC\n",
    "value_loss='smooth_l1' # 'smooth_l1' 'l2'; loss function to be used with the value function loss\n",
    "gamma = 0.99  # discount factor\n",
    "polyak = 0.995 # lambda for generalised advantage estimation\n",
    "\n",
    "# Model\n",
    "layers_config = [512, 256, 128, 64, 32]  # Number of units per layer in the network\n",
    "\n",
    "# GYM\n",
    "scenario_name = \"HumanoidStandup-v4\"\n",
    "max_steps = 400\n",
    "\n",
    "env = GymEnv(\n",
    "    env_name=scenario_name,\n",
    "    device=gym_device,\n",
    ")\n",
    "\n",
    "# transform the environment to a float32 tensor\n",
    "env = TransformedEnv(\n",
    "    env,\n",
    "    transform=DoubleToFloat(),\n",
    "    device=gym_device,\n",
    ")\n",
    "\n",
    "check_env_specs(env)\n",
    "\n",
    "time = time.time()\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observation_spec\", env.observation_spec)\n",
    "print(\"oberservation_space\", env.observation_space)\n",
    "print(\"action_spec\", env.action_spec)\n",
    "print(\"action_space\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_policy_module(env, layers_config, device):\n",
    "    policy_net = MLP(\n",
    "        in_features=env.observation_space.shape[0],\n",
    "        out_features=env.action_space.shape[0] * 2,  # 2 outputs per action: loc and scale\n",
    "        device=device,\n",
    "        depth=len(layers_config),\n",
    "        num_cells=layers_config,\n",
    "        activation_class=torch.nn.LeakyReLU,\n",
    "    )\n",
    "\n",
    "    seq_policy_net = torch.nn.Sequential(\n",
    "        policy_net,\n",
    "        NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    "    )\n",
    "    \n",
    "    policy_module = TensorDictModule(\n",
    "        seq_policy_net,\n",
    "        in_keys=[\"observation\"],\n",
    "        out_keys=[\"loc\", \"scale\"],\n",
    "    )\n",
    "\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_module,\n",
    "        in_keys=[\"loc\", \"scale\"],\n",
    "        spec=env.action_spec,\n",
    "        distribution_class=TanhNormal,\n",
    "        distribution_kwargs={\n",
    "            \"min\": env.action_spec.space.minimum,\n",
    "            \"max\": env.action_spec.space.maximum,\n",
    "            \"tanh_loc\": False,\n",
    "        },\n",
    "        default_interaction_type=InteractionType.RANDOM,\n",
    "        return_log_prob=False,\n",
    "    )\n",
    "\n",
    "    return policy\n",
    "\n",
    "def make_Qval_module(env, layers_config, device):\n",
    "    Qval_net = MLP(\n",
    "        in_features=env.observation_space.shape[0] + env.action_space.shape[0],\n",
    "        out_features=1,  # 2 outputs per action: loc and scale\n",
    "        device=device,\n",
    "        depth=len(layers_config),\n",
    "        num_cells=layers_config,\n",
    "        activation_class=torch.nn.LeakyReLU,\n",
    "    )\n",
    "\n",
    "    Qval = ValueOperator(\n",
    "        module=Qval_net,\n",
    "        in_keys=[\"observation\", \"action\"],\n",
    "    )\n",
    "\n",
    "    return Qval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collector(env, policy, device, storing_device, frames_per_batch, init_random_frames, total_frames, max_steps_per_traj=1000):\n",
    "    return SyncDataCollector(\n",
    "        env,\n",
    "        policy,\n",
    "        device=device,\n",
    "        storing_device=storing_device,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        init_random_frames=init_random_frames,\n",
    "        total_frames=total_frames,\n",
    "        max_frames_per_traj=max_steps_per_traj,\n",
    "    )\n",
    "\n",
    "def make_replay_buffer(frames_per_batch, minibatch_size, device):\n",
    "    return ReplayBuffer(\n",
    "        storage=LazyTensorStorage(\n",
    "            frames_per_batch, device=device\n",
    "        ),  # We store the frames_per_batch collected at each iteration\n",
    "        sampler=SamplerWithoutReplacement(),\n",
    "        batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loss_module(env, policy, Qval, value_loss, gamma, polyak):\n",
    "    loss_module = SACLoss(\n",
    "        actor_network=policy,\n",
    "        qvalue_network=Qval,\n",
    "        loss_function=value_loss,\n",
    "        action_spec=env.action_spec,\n",
    "    )\n",
    "    loss_module.make_value_estimator(gamma=gamma)\n",
    "    target_net_updater = SoftUpdate(loss_module,\n",
    "                                    eps=polyak)\n",
    "    \n",
    "    return loss_module, target_net_updater\n",
    "\n",
    "def make_sac_optimizers(loss_module, lr_actor, lr_critic):\n",
    "    critic_params = list(loss_module.qvalue_network_params.flatten_keys().values())\n",
    "    actor_params = list(loss_module.actor_network_params.flatten_keys().values())\n",
    "\n",
    "    optimizer_actor = torch.optim.Adam(\n",
    "        actor_params,\n",
    "        lr=lr_actor,\n",
    "    )\n",
    "    optimizer_critic = torch.optim.Adam(\n",
    "        critic_params,\n",
    "        lr=lr_critic\n",
    "    )\n",
    "    optimizer_alpha = torch.optim.Adam(\n",
    "        [loss_module.log_alpha],\n",
    "        lr=3.0e-4,\n",
    "    )\n",
    "    return optimizer_actor, optimizer_critic, optimizer_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    loss_module,\n",
    "    optimizers, # (optim_actor, optim_critic, optim_alpha)\n",
    "    target_net_updater,\n",
    "    collector,\n",
    "    replay_buffer,\n",
    "    n_iters,\n",
    "    num_epochs,\n",
    "    frames_per_batch,\n",
    "    minibatch_size):\n",
    "\n",
    "    # Main loop\n",
    "    pbar = tqdm(total=n_iters, desc=\"Training\")\n",
    "\n",
    "    episode_reward_mean_list = []\n",
    "\n",
    "    optim_actor, optim_critic, optim_alpha = optimizers\n",
    "\n",
    "    for tensordict_data in collector:\n",
    "        # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "        data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "        replay_buffer.extend(data_view)\n",
    "\n",
    "        for _ in range(num_epochs):\n",
    "            for _ in range(frames_per_batch // minibatch_size):\n",
    "                subdata = replay_buffer.sample()\n",
    "\n",
    "                loss_vals = loss_module(subdata)\n",
    "\n",
    "                loss_actor = loss_vals[\"loss_actor\"]\n",
    "                loss_qval = loss_vals[\"loss_qvalue\"]\n",
    "                loss_alpha = loss_vals[\"loss_alpha\"]\n",
    "\n",
    "                optim_actor.zero_grad()\n",
    "                loss_actor.backward()\n",
    "                optim_actor.step()\n",
    "\n",
    "                optim_critic.zero_grad()\n",
    "                loss_qval.backward()\n",
    "                optim_critic.step()\n",
    "\n",
    "                optim_alpha.zero_grad()\n",
    "                loss_alpha.backward()\n",
    "                optim_alpha.step()\n",
    "\n",
    "                # update qnet_target params\n",
    "                target_net_updater.step()\n",
    "\n",
    "        # update weights of the inference policy\n",
    "        collector.update_policy_weights_()\n",
    "        \n",
    "        # Logging\n",
    "        done = (\n",
    "            tensordict_data[\"next\", \"done\"]\n",
    "            if tensordict_data[\"next\", \"done\"].any()\n",
    "            else tensordict_data[\"next\", \"truncated\"]\n",
    "        )\n",
    "        episode_reward_mean = (\n",
    "            tensordict_data.get((\"next\", \"reward\"))[done].mean().item()\n",
    "        )\n",
    "        episode_reward_mean_list.append(episode_reward_mean)\n",
    "        pbar.set_description(f\"episode_reward_mean = {round(episode_reward_mean, 4)}\", refresh=False)\n",
    "        pbar.update()\n",
    "\n",
    "    pbar.close()\n",
    "    collector.shutdown()\n",
    "    replay_buffer.empty()\n",
    "    \n",
    "    return episode_reward_mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = make_policy_module(env, layers_config, device)\n",
    "Qval = make_Qval_module(env, layers_config, device)\n",
    "collector = make_collector(env, policy, device, gym_device, frames_per_batch, init_random_frames=0, total_frames=total_frames, max_steps_per_traj=max_steps)\n",
    "replay_buffer = make_replay_buffer(frames_per_batch, minibatch_size, device)\n",
    "loss_module, target_net_updater = make_loss_module(env, policy, Qval, value_loss, gamma, polyak)\n",
    "optimizer_actor, optimizer_critic, optimizer_alpha = make_sac_optimizers(loss_module, lr_actor, lr_critic)\n",
    "episode_reward_mean_list = training_loop(loss_module, (optimizer_actor, optimizer_critic, optimizer_alpha), target_net_updater, collector, replay_buffer, n_iters, num_epochs, frames_per_batch, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.savefig('episode_reward_mean'+str(time)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torchvision\n",
    "!pip3 install av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.record import VideoRecorder\n",
    "from torchrl.record.loggers.csv import CSVLogger\n",
    "from torchrl.envs import TransformedEnv\n",
    "import pyvirtualdisplay\n",
    "\n",
    "display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "env = GymEnv(\n",
    "    env_name=scenario_name,\n",
    "    device=gym_device,\n",
    "    from_pixels=True,\n",
    "    pixels_only=False,\n",
    ")\n",
    "\n",
    "logger = CSVLogger(exp_name=scenario_name, log_dir=scenario_name+\"_policy\", video_format=\"mp4\") # log_dir is the directory where the videos will be saved\n",
    "\n",
    "vr_transformation = VideoRecorder(logger=logger, out_keys='obs', tag=\"run_video\")\n",
    "df_transformation = DoubleToFloat()\n",
    "\n",
    "env = TransformedEnv(env, vr_transformation)\n",
    "env = TransformedEnv(env, df_transformation)\n",
    "                                \n",
    "n_rollout_steps = max_steps\n",
    "with torch.no_grad():\n",
    "    rollout = env.rollout(n_rollout_steps,\n",
    "        policy=policy,\n",
    "        auto_cast_to_device=True,\n",
    "        break_when_any_done=False,)\n",
    "vr_transformation.dump()\n",
    "# read mp4 video\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "video = io.open(scenario_name+\"_policy\"+\"/\"+scenario_name+'/videos/run_video_0.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
